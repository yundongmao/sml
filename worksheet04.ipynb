{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 4\n",
    "## Perceptron\n",
    "***\n",
    "In this worksheet, we'll implement the perceptron (a building block of neural networks) from scratch. \n",
    "Our key objectives are:\n",
    "\n",
    "* to review the steps involved in the perceptron training algorithm, \n",
    "* to assess how the perceptron behaves in two distinct scenarios (separable vs. non-separable data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Synthetic data set\n",
    "We'll use the built-in `make_classification` function from `sklearn` to generate a synthetic binary classification data set.\n",
    "The main advantage of using synthetic data is that we have complete control over the distribution. \n",
    "This is useful for studying machine learning algorithms under specific conditions.\n",
    "In particular, we'll be varying the *degree of separability* between the two classes by adjusting the `class_sep` parameter below.\n",
    "To begin, we'll work with a data set that is almost linearly separable (with `class_sep = 2`).\n",
    "\n",
    "**Note:** In this worksheet we deviate from the standard `0`/`1` encoding for binary class labels used in `sklearn`. \n",
    "We use `-1` in place of `0` for the \"negative\" class.\n",
    "This matches the presentation in lectures and makes the gradient descent update simpler to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sep = 2 \n",
    "#class_sep = 0.5 # uncomment this later\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "X, Y = make_classification(n_samples=200, n_features=2, n_informative=2, \n",
    "                           n_redundant=0, n_clusters_per_class=1, flip_y=0,\n",
    "                           class_sep=class_sep, random_state=1)\n",
    "Y[Y==0] = -1 # encode \"negative\" class using -1 rather than 0\n",
    "plt.plot(X[Y==-1,0], X[Y==-1,1], \"o\", label=\"Y = -1\")\n",
    "plt.plot(X[Y==1,0], X[Y==1,1], \"o\", label=\"Y = 1\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is the perceptron a suitable classifier for this data set?\n",
    "  \n",
    "In preparation for training and evaluating a perceptron on this data, we randomly partition the data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Definition of the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lectures that a perceptron is a binary classifier which maps an input vector $\\mathbf{x}$ to a binary ouput given by\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x}; \\mathbf{w}, b) &= \\begin{cases}\n",
    "    1 & \\mathrm{if} \\ s(\\mathbf{x}; \\mathbf{w}, b) \\geq 0, \\\\\n",
    "    -1 & \\mathrm{otherwise},\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "where $s(\\mathbf{x}; \\mathbf{w}, b) = \\mathbf{w} \\cdot \\mathbf{x} + b$. \n",
    "Here $\\mathbf{w}$ is a vector of weights (one for each feature) and $b$ is the bias term.\n",
    "\n",
    "Let start by implementing the weighted sum function $s(\\mathbf{x}; \\mathbf{w}, b)$ below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_sum(X, w, b):\n",
    "    \"\"\"\n",
    "    Returns an array containing the weighted sum s(x) for each instance x in the feature matrix\n",
    "    \n",
    "    Arguments:\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    w : numpy array, shape: (n_features,)\n",
    "        weights vector\n",
    "    b : float\n",
    "        bias term\n",
    "    \"\"\"\n",
    "    return ... # fill in\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Returns an array containing the predicted binary labels (-1/1) for each instance in the feature matrix\n",
    "    \n",
    "    Arguments:\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    w : numpy array, shape: (n_features,)\n",
    "        weights vector\n",
    "    b : float\n",
    "        bias term\n",
    "    \"\"\"\n",
    "    return np.where(weighted_sum(X, w, b) >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perceptron training algorithm\n",
    "We're now going to implement the perceptron training algorithm presented in lectures.\n",
    "The algorithm is essentially an application of *sequential gradient descent* (a.k.a. *online stochastic gradient descent*) to minimise the following empirical loss:\n",
    "$$\n",
    "L[\\mathbf{w}, b] = \\frac{1}{n} \\sum_{i = 1}^{n} \\max(0, -y \\cdot s(\\mathbf{x}; \\mathbf{w}, b))\n",
    "$$\n",
    "It's *sequential* in that the weights/bias are updated for each training instance—one at a time.\n",
    "After iterating through all of the training instances, we say that we've completed an *epoch*.\n",
    "Typically, multiple epochs are required to get close to the optimal solution.\n",
    "\n",
    "Let's now write a function called `train` which implements sequential gradient descent.\n",
    "For your reference, the function should implement the following pseudocode:\n",
    "\n",
    "> repeat $n_\\mathrm{epochs}$ times\n",
    "\n",
    "> >   for each $(\\mathbf{x}, y)$ pair in the training set\n",
    "\n",
    "> > > if the model prediction $\\hat{y} = f(\\mathbf{x})$ and $y$ differ, make a weight update\n",
    "\n",
    "> return $\\mathbf{w}$ and $b$\n",
    "\n",
    "**Note:** the weight update in the inner-most loop is given by $\\mathbf{w} \\gets \\mathbf{w} + \\eta y  \\mathbf{x}$ and $b \\gets b + \\eta y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, Y, n_epochs, w, b, eta=0.1):\n",
    "    \"\"\"\n",
    "    Returns updated weight vector w and bias term b\n",
    "    \n",
    "    Arguments:\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    Y : numpy array, shape: (n_instances,)\n",
    "        target class labels relative to X\n",
    "    n_epochs : int\n",
    "        number of epochs (full sweeps through X)\n",
    "    w : numpy array, shape: (n_features,)\n",
    "        initial guess for weights vector\n",
    "    b : float\n",
    "        initial guess for bias term\n",
    "    eta : positive float\n",
    "        step size (default: 0.1)\n",
    "    \"\"\"\n",
    "    for t in range(n_epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            yhat = ... # fill in\n",
    "            if yhat != Y[i]:\n",
    "                w += ... # fill in\n",
    "                b += ... # fill in\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running it for 5 epochs.\n",
    "You should get the following result for the weights and bias term:\n",
    "`w = [ 0.26746342 -0.96011853]; b = -0.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise weights and bias to zero\n",
    "w = np.zeros(X.shape[1]); b = 0.0\n",
    "\n",
    "w, b = train(X_train, Y_train, 5, w, b)\n",
    "print(\"w = {}; b = {}\".format(w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the perceptron, let's see how it performs.\n",
    "\n",
    "Below we plot the data (training and test sets) along with the decision boundary (which is defined by $\\{\\mathbf{x}: s(\\mathbf{x}; \\mathbf{w}, b) = 0$\\}).\n",
    "\n",
    "**Note:** It's not necessary to understand how the `plot_results` function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(X_train, Y_train, X_test, Y_test, score_fn, threshold = 0):\n",
    "    # Plot training set\n",
    "    plt.plot(X_train[Y_train==-1,0], X_train[Y_train==-1,1], \"o\", label=\"Y=-1, train\")\n",
    "    plt.plot(X_train[Y_train==1,0], X_train[Y_train==1,1], \"o\", label=\"Y=1, train\")\n",
    "    plt.gca().set_prop_cycle(None) # reset colour cycle\n",
    "\n",
    "    # Plot test set\n",
    "    plt.plot(X_test[Y_test==-1,0], X_test[Y_test==-1,1], \"x\", label=\"Y=-1, test\")\n",
    "    plt.plot(X_test[Y_test==1,0], X_test[Y_test==1,1], \"x\", label=\"Y=1, test\")\n",
    "\n",
    "    # Compute axes limits\n",
    "    border = 1\n",
    "    x0_lower = X[:,0].min() - border\n",
    "    x0_upper = X[:,0].max() + border\n",
    "    x1_lower = X[:,1].min() - border\n",
    "    x1_upper = X[:,1].max() + border\n",
    "\n",
    "    # Generate grid over feature space\n",
    "    resolution = 0.01\n",
    "    x0, x1 = np.mgrid[x0_lower:x0_upper:resolution, x1_lower:x1_upper:resolution]\n",
    "    grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "    s = score_fn(grid).reshape(x0.shape)\n",
    "\n",
    "    # Plot decision boundary (where s(x) == 0)\n",
    "    plt.contour(x0, x1, s, levels=[0], cmap=\"Greys\", vmin=-0.2, vmax=0.2)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"$x_0$\")\n",
    "    plt.ylabel(\"$x_1$\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_results(X_train, Y_train, X_test, Y_test, lambda X: weighted_sum(X, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How well does the decision boundary separate the points in the two classes? Where do you think the decision boundary should go? \n",
    "\n",
    "To evaluate the perceptron quantitatively, we'll use the error rate (proportion of misclassified instances).\n",
    "The error rate is a reasonable evaluation measure to use for this data since the classes are well balanced.\n",
    "\n",
    "Complete the `evaluate` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, Y, w, b):\n",
    "    \"\"\"\n",
    "    Returns the proportion of misclassified instances (error rate)\n",
    "    \n",
    "    Arguments:\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    Y : numpy array, shape: (n_instances,)\n",
    "        target class labels relative to X\n",
    "    w : numpy array, shape: (n_features,)\n",
    "        weights vector\n",
    "    b : float\n",
    "        bias term\n",
    "    \"\"\"\n",
    "    return ... # fill in\n",
    "\n",
    "print(evaluate(X_train, Y_train, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block above computes the error rate on the training data, which is not a good idea in general. (Why?)\n",
    "\n",
    "Compute the error rate on the test set instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(..., ..., w, b)) # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does this compare to the error on the training set? Is it what you expected?\n",
    "\n",
    "Let's now examine how the train/test error rates vary as a function of the number of epochs.\n",
    "Note that careful tuning of the learning rate is needed to get sensible behaviour.\n",
    "Setting $\\eta(t) = \\frac{1}{1+t}$ where $t$ is the epoch number often works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hat = np.zeros(X_train.shape[1]); b_hat = 0\n",
    "n_epochs = 100\n",
    "\n",
    "# Initialize arrays to store errors for each epoch\n",
    "train_error = np.empty(n_epochs)\n",
    "heldout_error = np.empty(n_epochs)\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    # here we use a learning rate, which decays with each epoch\n",
    "    eta = 1./(1+t)\n",
    "    w_hat, b_hat = train(X_train, Y_train, 1, w_hat, b_hat, eta=eta)    \n",
    "    train_error[t] = evaluate(X_train, Y_train, w_hat, b_hat)\n",
    "    heldout_error[t] = evaluate(X_test, Y_test, w_hat, b_hat)\n",
    "\n",
    "plt.plot(train_error, label = 'Train error')\n",
    "plt.plot(heldout_error, label = 'Test error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch, $t$')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does the test error track the training error closely? \n",
    "\n",
    "**Question:** Has the model changed significantly after training for more epochs (i.e. more than 5)? You can plot the new decision boundary by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(X_train, Y_train, X_test, Y_test, lambda X: weighted_sum(X, w_hat, b_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Repeat with class overlap\n",
    "By now you've probably concluded that the perceptron performs well on this data (`class_sep=2`), which is to be expected as it's roughly linearly separable. \n",
    "However, we know (from lectures) that the perceptron can fail on non-linearly separable data.\n",
    "To test this scenario, re-generate the synthetic data set with `class_sep=0.5` and repeat Sections 2–4.\n",
    "\n",
    "**Question:** What do you find? Pay particular attention to plot of the error error vs. training epochs. Do you notice anything unusual?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Comparison with logistic regression\n",
    "We've seen that the perceptron is not robust to binary classification problems with overlapping classes. \n",
    "But how does logistic regression fare in this case?\n",
    "\n",
    "Run the code block below to fit a logistic regression model using `sklearn`. \n",
    "You may wish to switch off regularisation (alter the `C` parameter) for a fairer comparison with the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(X_train, Y_train, X_test, Y_test, lambda X: clf.decision_function(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does the logistic regression classifier compare to the perceptron?\n",
    "\n",
    "**Question:** Compute the error rate for the logistic regression classifier and compare it to the error rate for the perceptron (for `class_sep=0.5`). *Hint: you may wish to use `clf.score(...)`*\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... # fill in"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
